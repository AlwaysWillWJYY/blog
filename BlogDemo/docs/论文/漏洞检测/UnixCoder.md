### VulDeeLocator: A Deep Learning-based Fine-grained Vulnerability Detector，TDSC(A) 2021 。

本文提出一个针对C语言的新的漏洞检测模型`VulDeeLocator`，将同一工程下的多个文件通过define-use的关系连接起来，并用中间代码表示替换源代码表示。将`BRNN`模型扩展为`BRNN-vdl`模型，不仅能够检测漏洞，还能够将漏洞出现的位置限制在几行之内。本文提出了一个新的数据集，每个样例都由`LLVM`的中间代码和源代码一同构成，在合成数据集和真实数据集上都验证了`VulDeeLocator`的有效性

* 跨语言模型，单双模态进行漏洞检测

* `GNN`表示
* 输出不仅是`mlp`，也可以定位到行

 sudo vi /etc/hosts 

104.244.46.208  github.global.ssl.fastly.net

108.160.170.39  assets-cdn.github.com 

 service networking restart 



#### 摘要 

编程语言的预训练模型最近在代码智能方面取得了巨大成功。为了支持与代码相关的理解和生成任务，最近的工作尝试预训练统一的编码器-解码器模型。然而，这种编码器-解码器框架对于自回归任务来说是次优的，尤其是对于需要解码器才能进行有效推理的代码完成。在本文中，我们提出了UniXcoder，一种用于编程语言的统一跨模态预训练模型。该模型利用带有前缀适配器的掩码注意矩阵来控制模型的行为，并利用AST和代码注释等跨模式内容来增强代码表示。为了对表示为树的AST进行并行编码，我们提出了一种一对一映射方法，将AST转换为保留树中所有结构信息的序列结构。此外，我们建议利用多模态内容通过对比学习学习代码片段的表示，然后使用跨模态生成任务在编程语言之间对齐表示。我们在九个数据集上评估UniXcoder的五个代码相关任务。为了进一步评估代码片段表示的性能，我们还构建了一个新任务的数据集，称为零镜头代码到代码搜索。结果表明，我们的模型在大多数任务中都达到了最先进的性能，分析表明，注释和AST都可以增强UniXcoder。 

### 介绍

预训练模型，如GPT（Radford等人，2018）和BERT（Devlin等人，2018年），在许多自然语言处理（NLP）任务中大大提高了技术水平。

这些预先训练的模型是预先训练的具有自我监督目标的大量文本数据，并且可以进行微调以适应下游任务。受NLP中预训练模型的成功启发，已经提出了用于编程语言（PL）的预训练模型（Kanade等人，2019；Feng等人，2020；Svyatkovskiy等人，2020），以促进代码智能的发展。Svyatkovskiy等人（2020）提出了GPT-C，该GPT-C采用从左到右的变压器（V aswani等人，2017）来支持生成任务，如代码完成，但单向框架对于理解任务来说是次优的。相比之下，其他工作（Kanade等人，2019；Feng等人，2020）在源代码上预训练了双向Transformer编码器，这显著提高了代码相关理解任务的性能。

然而，当应用于生成任务时，它的双向性需要额外的解码器，在这种情况下，解码器从头开始初始化，无法从预训练中受益。

 在这项工作中，我们介绍了UniXcoder，这是一种统一的跨模态预训练编程语言模型，用于支持与代码相关的理解和生成任务。UniXcoder基于多层Transformer，遵循Dong等人。  利用带有前缀适配器的掩码注意矩阵来控制对每个令牌的上下文的访问。在代码智能方面，与当前的统一编码器模型（Ahmad等人，2021；Wang等人，2021）相比，UniXcoder可以更好地应用于自动回归任务，例如代码完成，在实践中需要仅使用解码器的方式来执行有效的推理。除了将代码作为唯一的输入，我们还考虑了多模式内容，如代码注释和抽象语法树（AST），以增强代码表示。通常，用户编写的代码注释提供关于源代码的关键语义信息，如“排序给定列表”，AST包含丰富的语法信息，如语句类型和语句之间的嵌套关系，这有助于  模型更好地理解源代码。为了对表示为树的AST进行并行编码，我们提出了一种一对一映射方法，将AST转换为保留树的所有信息的序列结构，然后将序列用作输入以增强代码表示。 

我们使用三种类型的语言建模任务预训练UniXcoder：掩蔽语言建模（Devlin等人，2018）、单向语言建模（Radford等人，2018年）和去噪目标（Raffel等人，2019年），这可以使模型支持各种类型的下游任务。此外，我们引入了两个预训练任务来学习可以表示代码片段语义的嵌入。一种是利用AST增强代码片段嵌入语义的多模态对比学习，另一种是跨模态生成，利用代码注释在编程语言之间对齐嵌入。 



我们在九个公共数据集上评估UniXcoder的五个任务，包括两个理解任务：克隆检测和代码搜索，两个生成任务：代码摘要和代码生成，以及一个自回归任务：代码完成。为了进一步测试代码片段嵌入，我们提出了一项新任务，称为零镜头代码到代码搜索，并从CodeNet语料库（Puri等人，2021）中为此任务构建了一个新的数据集。实验结果表明，我们的模型在大多数任务中都达到了最先进的性能。进一步分析表明，AST和代码注释都可以增强UniXcoder以更好地捕获代码语义。

总之，本文的贡献是：（1）我们提出了一个利用多模态内容的统一跨模态预训练模型，即。代码注释和AST，以支持代码相关的理解、生成任务和自动回归任务。（2） 我们提出了一个一对一映射函数，它将AST转换为保留AST所有信息的序列，并可以用源代码和注释并行编码。（3） 我们进一步建议利用代码注释来学习代码片段表示，并构建一个新的零镜头代码搜索数据集，以评估代码片段表示的质量。（4） 实验结果表明，UniXcoder在大多数下游任务上都有显著改进。

### 相关工作

随着自然语言（NL）处理中预训练的巨大成功（Devlin等人，2018；Lewis等人，2019；Raffel等人，2018；Brown等人，2020），已经提出了编程语言的预训练模型，以促进代码智能的发展。这些预训练模型通常可分为三类：编码器模型、仅解码器模型和编码器-解码器模型。

仅编码模型（Kanade等人，2019；Buratti等人，2020；Feng等人，2020年；Guo等人，2020，Wang等人，2022）预训练双向变压器，其中每个令牌可以相互关注。

Kanade等人（2019）通过掩蔽语言建模和下一句预测目标，在Python源代码语料库上预训练CuBERT。

CodeBERT（Feng et al.，2020）在六种编程语言中的NLPL对上进行了预训练，并具有新的预训练任务，即替换令牌检测。

GraphCodeBERT（Guo等人，2020）利用数据流增强代码表示，而SYNCOBERT（Wang等人，2022）通过AST边缘预测和对比学习结合了抽象语法树。然而，仅编码器的模型需要额外的解码器来执行生成任务，在这种情况下，解码器从头开始初始化，无法从预训练中受益。

至于仅限解码器的预训练模型，Sviatkovskiy等人（2020）和Lu等人（2021）分别提出了GPT-C和CodeGPT，这两种模型都是使用单向语言建模进行预训练的，只允许令牌参与之前的令牌，并允许令牌本身预测下一个令牌。

只有解码器的模型擅长于像代码完成这样的自回归任务，但单向框架对于理解任务来说是次优的。

最近的一些工作探索了编码器-解码器模型，以支持理解和生成任务。PLBART（Ahmad等人，2021）基于BART（Lewis等人，2019）架构，并使用去噪目标对NL和PL语料库进行预训练。CodeT5（Wang等人，2021）采用了T5（Raffel等人，2019）模型，该模型考虑了来自标识符的关键令牌类型信息，并允许对下游任务进行多任务学习。

TreeBERT（姜等人，2021）遵循编码器-编码器-变压器框架，但通过建模AST路径来利用树结构信息。

 与当前的统一模型不同，UniXcoder基于多层Transformer，并使用带有前缀适配器的掩码注意矩阵 以控制模型的行为以支持理解和生成任务。与编码器-解码器架构相比，UniXcoder可以更好地应用于自回归任务，如IDE中广泛使用的代码完成，因为该任务需要仅解码器的方式来执行有效的推理。Liu等人。

（2020）还用多任务学习预训练了类似的模型CugLM，但它们只关注代码完成，而不是各种任务。此外，我们通过一对一映射函数将AST中的语法信息转换为序列，以增强代码表示。与以前使用AST的预训练模型不同，映射函数保留了来自AST的所有结构信息，并且不需要额外的预训练任务（例如边缘预测）来隐式学习AST结构。



### UniXcoder

 在本节中，我们描述了UniXcoder，一种统一的跨模态预训练模型，它利用多模态数据（即代码注释和AST）来预训练代码表示。该模型基于Transformer，并使用带有前缀适配器的掩码注意矩阵（Dong等人，2019）来控制模型的行为。在下文中，我们首先介绍如何将多模态数据统一为UniXcoder的输入（§3.1），然后介绍模型架构（§3.2）和预训练任务 

#### Input Representation

 我们在图1中给出了一个带有注释和AST的python代码示例 可以看到，注释“返回数据的样本算术平均值”高度描述了源代码的功能，它提供了有关源代码的关键语义信息。此外，AST提供了丰富的语法信息，例如子树“参数→ “（数据）”表示函数定义中术语（数据）的类型（即参数）。它们都可以用作附加知识，以增强预训练模型中的代码表示。然而，AST通常表示为树，不能直接用作Transformer的输入。为了将AST与代码注释并行编码，我们提出了算法1中描述的一对一映射函数F，以将AST转换为保留所有结构信息的序列。 

 特别地，给定AST的根节点根，该算法递归地将相同的函数F应用于其子节点，然后在其两侧添加两个特殊后缀（分别为lef t和right）（算法1的第6-11行）。如果根节点是叶，我们直接生成它的名称（第45行）。取“参数→ 例如，映射函数F将子树转换为“＜parameters，left＞（数据）＜parameter，right＞”。 可以有多种方法将树转换为标记序列，例如预排序遍历。

然而，特定的转换应该是一对一映射函数。否则，映射可能会将树与另一个结构混淆。我们的映射函数F满足这一要求（见附录A的证明）。最后，给定源代码C，我们将其注释W={w0，w1，…，wm−1} 和平坦的AST 标记序列F（T（C））={c0，c1，…，ck−1} 其中T（C）是代码的AST的根。对于输入格式，我们将它们与一个前缀连接起来作为输入序列，如图2底部所示，其中前缀表示模型的工作模式，下面将讨论 

#### Model Architecture

图2显示了UniXcoder的模型架构。该模型在代码注释上应用N个变换器层，并用前缀平坦化AST，以产生隐藏状态HN={hN0，hN1，…，hNn−1} ，其中前缀p∈ {[Enc]、[Dec]、[22D]}表示模型的行为，例如[E2D]表示UniXcoder作为编码器-解码器模型工作。

每个变压器层包含一个结构相同的变压器，该变压器使用多头自关注操作（V aswani et al.，2017），然后在前一层的输出上使用前馈层。对于第l个变压器层，多头自关注的输出通过 其中前一层的输出Hl−1.∈ Rn×dh分别线性映射到查询、键和值的三元组。dk是头部的尺寸，M∈ Rn×n是一个掩码矩阵，用于控制令牌在计算其上下文表示时可以处理的上下文，如图2中间所示。如果允许第i个令牌处理第j个令牌，则Mij设置为0，否则−∞ 

对于仅编码器模式，我们在输入前面添加一个特殊标记[Enc]作为前缀，并将掩码矩阵的所有元素设置为0，以允许所有标记相互关注。对于仅解码器模式，使用前缀[Dec]，掩码的上三角部分设置为−∞ 以指示每个令牌只能关注自身和先前令牌。对于编码器-解码器模式，允许源输入中的令牌相互关注，而目标输入中的符号只关注自身以及源和目标输入中之前的令牌。我们使用[E2D]前缀来表示UniXcoder作为编码器-解码器模型工作。在预训练阶段，模型参数以不同的模式共享，并根据多个目标进行优化，以支持各种类型的下游任务,

### Pre-training Tasks

我们将在本节中描述UniXcoder中使用的预训练任务。如图2右侧所示，我们首先使用三个任务预训练UniXcoder，包括掩蔽语言建模（Devlin等人，2018）、单向语言建模（Radford等人，2018年）和去噪目标（Raffel等人，2019年）。这些任务针对不同的模式设计，使UniXcoder能够支持各种类型的与代码相关的下游任务。然后，我们建议利用多模态数据通过对比学习和跨模态生成来学习代码片段嵌入，如图3所示。 

 掩码语言建模对于仅编码器模式，我们遵循Devlin等人（2018）应用掩码语言建模（MLM）预训练任务。特别地，我们从输入序列中抽取15%的标记Sm，然后用[MASK]（随机）标记替换其中的80%（10%），并保留另外10%的标记不变。任务是根据双向上下文标记预测掩码标记的原始标记，如图2（a）所示。特别地，模型可以利用来自注释的语义信息和来自AST的语法信息来推断屏蔽代码标记，这鼓励模型从不同的知识资源学习代码表示。目标按等式3计算，其中Xmask是屏蔽的输入序列.

 单向语言建模我们使用单向语言建模（ULM）预训练任务来预训练仅解码器模式，以支持代码完成等自回归任务，如图2（b）所示。该任务根据之前的令牌和自身{x0，x1，…，xi逐一预测下一个令牌xi−1} ，这可以使用三角形矩阵进行关注掩模。 

去噪目标去噪（DNS）预训练目标已被证明对NLP中的BART（Lewis et al.，2019）和T5（Raffel et al.）等编码器-解码器模型非常有效。该任务随机屏蔽任意长度的跨度，然后在编码器-编码器模式下生成这些屏蔽的跨度。更好地支持生成任务.

与代码摘要一样，我们对编码器-解码器模式使用与T5相似的去噪目标，如图2（c）所示。特别地，我们首先将输入序列分成最大（bn×rl c，1）个块，然后为每个块随机屏蔽1到2l-1个标记的跨度，其中n是输入的长度，r是损坏率，l是屏蔽跨度的平均长度。我们分别将腐败率设置为15%，平均长度设置为5。连接{y0，y1，…，yn−1} 在第k个跨距前面带有特殊标记[M ASKk]的所有掩码跨距中，将用作输出：

 代码片段表示学习:

除了针对不同模式设计的上述三个预训练任务之外，我们建议利用多模式数据来学习代码片段Ci的语义嵌入ehi。如图3所示，我们首先使用UniXcoder对映射的AST序列进行编码，然后在源输入的隐藏状态上应用平均池层，以获得语义嵌入ehi。为了学习语义嵌入，我们提出了两个预训练任务。

一种是多模态对比学习（MCL），另一种是跨模态生成（CMG）。 对于多模式对比学习，我们遵循Gao等人（2021）使用不同的隐藏退出掩码转发相同的输入作为正例eh+i，并使用同一批次中的其他表示作为反例。损失按等式6计算，其中b为批量，τ为温度超参数，cos（·，·）为两个向量之间的余弦相似性。 

对于跨模态生成，我们要求模型生成其注释W={w0，w1，…，wm−1}.

注释描述了代码的功能，这不仅可以帮助模型理解代码语义，还可以通过统一的自然语言描述作为支点，在不同的编程语言之间对齐表示。由于注释的生成取决于 代码，它将迫使模型将注释中的语义信息融合到代码的隐藏状态中。损失按等式7计算，其中X是平坦的AST令牌序列 .

 为了学习自然语言的语义嵌入，我们以50%的概率随机交换源输入和目标输入.考虑到在下游任务中显式添加AST将带来额外的成本，如解析时间和增加输入长度（标记化后输入长度增加70%），我们通过预训练隐式地从AST学习知识，并仅在微调阶段保留AST的叶子（即源代码）。可以通过在预训练阶段以50%的概率随机丢弃AST的所有非终端符号来缓解该差距。有关预训练数据集和设置的更多详细信息.

### Experiments

 我们在九个公共数据集上对UniXcoder进行了五项任务评估，包括两项理解任务（§4.2）、两项生成任务（§4.3）和一项自回归任务（§4.4）。为了进一步评估代码片段嵌入的性能，我们还提出了一个新任务，称为零炮代码到代码搜索（§4.5）。有关数据集和微调的更多详细信息，请参见附录C 

 我们将UniXcoder与最先进的预训练模型进行了比较，包括仅编码器、解码器和编码器-解码器模型 

 对于仅编码器的模型，我们考虑了Roberta（Liu等人，2019）用MLM在文本语料库上预训练，CodeBERT（Feng等人，2020）在NL-PL对上使用MLM和替换标记检测预训练，GraphCodeBERT（Guo等人，2020年）利用数据流增强代码表示，SYNCOBERT结合了AST边缘预测和对比学习 

 对于纯解码器模型，我们考虑了GPT-2（Radford等人，2019）和CodeGPT（Lu等人，2021），其中前者在文本语料库上预训练，后者在CodeSearchNet数据集上预训练。两者都使用ULM作为目标。 





**UniXcoder: 统一的跨模式预训练模型**

Unixcoder是一种用于编程语言的统一的跨模式预训练模型。该模型利用带有前缀适配器的掩码注意矩阵来控制模型的行为，并利用AST和代码注释等跨模式内容来增强代码表示。为了对并行表示为树的AST进行编码，论文提出了一种一对一的映射方法，可以保留AST中所有结构信息的序列结构。该模型还利用多模态内容通过对比学习来学习代码片段的表示，然后使用跨模态生成任务来对齐编程语言之间的表示。







